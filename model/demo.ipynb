{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"a7bSTDiow_FA"},"outputs":[],"source":["try:\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","\n","import tensorflow as tf\n","import keras\n","import numpy as np\n","import pandas as pd\n","import jieba.posseg as pseg\n","import pickle\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dIAW0qWtAMj3"},"outputs":[],"source":["def jieba_tokenizer(text):\n","  words = pseg.cut(text)\n","  return ' '.join([word for word, flag in words if flag != 'x' and flag != 'eng'])\n","\n","with open('tokenizer.pickle', 'rb') as handle:\n","  tokenizer = pickle.load(handle)\n","\n","model = keras.models.load_model('my_model.h5')\n","pic = np.load(\"pic.npy\", allow_pickle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y-xByVM2-gI7"},"outputs":[],"source":["def demo(text):\n","\n","  test = pd.DataFrame(data={'Text':[text]})\n","  test['Text_tokenized'] = test.Text.apply(jieba_tokenizer)\n","  tokenizer.fit_on_texts(test.Text_tokenized)\n","  test = tokenizer.texts_to_sequences(test.Text_tokenized)\n","  test = keras.preprocessing.sequence.pad_sequences(test, maxlen=70)\n","\n","  predict = np.argmax(model.predict(test), axis=-1)\n","  plt.imshow(pic[int(predict)])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4o_QJQss3faO"},"outputs":[],"source":["demo(input())"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"demo.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
